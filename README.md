# Pentesting-Lab-with-Microsoft-Sentinel

The contents of this repository outline the construction of a virtual pentesting lab within Microsoft Azure, hosting an instance
of OWASP's Buggy Web Application, and the deployment and configuration of Azure's cloud-native SIEM, Microsoft Sentinel, to monitor the environment.

This document contains the following sections:


Network Topology Description                                              
Access Policies                                       
Deployment Steps Taken Within Microsoft Sentinel


To view a slide presentation of this project, you can find it under the folder "Presentation".

### **Network Topology**

![Topology](https://user-images.githubusercontent.com/129786707/236060350-c84ddd0c-2921-4151-af7b-a09a57ca0e00.png)

All resources, aside from my own computer, are within Microsoft Azure.

There are two resources with public IP addresses: our jump box provisioner and the load balancer for our web servers. For the purposes of this lab, my own IP address is whitelisted in our Network Security Group rules. (note: while whitelisting an IP address will only allow said IP authorized access to resources within the virtual network, this does not prevent attempts at unauthorized access to resources with a public-facing IP address, as part of this lab will demonstrate).

We have configured a ‘fan-in’ pattern for administrative access to our resources within our VNET. Our jump box is configured to only accept connections via secure-shell (SSH) from my own IP address, and the machines hosting our web application are configured to only accept SSH connections internally from our jump box. Since using SSH with a password is inherently weak, we have generated asymmetric key-pairs that only allow access to those holding the private keys, one on my computer for jump box access, and another within the Ansible provisioning node the jump box is hosting for accessing the internal servers.

The details of each machine are listed below:


|   Name        |  Function     |  IP Address  |  OS   |
| ------------ | ------------- | ----- | --- |
| Jump Box | Gateway | 10.0.0.4 | Linux Ubuntu |
| Web1 | Web Server | 10.0.0.5 | Linux Ubuntu |
| Web2 | Web Server | 10.0.0.6 | Linux Ubuntu |
| Web3 | Web Server | 10.0.0.7 | Linux Ubuntu |



Our Ansible container is configured as a provisioner for our web servers. This offers not only convenience from an administrative standpoint, as this allows us to configure our web servers automatically from a centralized location, but also ensures that each server will be configured identically with the same playbook. This concept is referred to as “infrastructure as code”.

The Ansible playbook used to configure the deployment of our Buggy Web Application via Docker can be found here:

![pentest.yml](assets/pentest.yml)

After running the playbook, we can SSH into any of our web servers and run a command listing our Docker Containers
to ensure they are deployed and running: 

![Deployed_BWAPP_Containers](https://user-images.githubusercontent.com/129786707/236060639-3fc59c45-ba36-4d7a-86ba-bd3d82fe1211.png)

The public IP address of the load balancer allows HTTP connections to the internal web servers (provided they are powered on with the Docker containers running). The load balancer is configured to distribute HTTP requests evenly between the three servers, ensuring a high level of availability as well as providing a layer of security from denial-of-service attacks.

We can enter the URL "http://<our-load-balancer-public-ip>/install.php" and confirm that our application is accessible:









